<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>RCT in schools</title>

<script src="site_libs/header-attrs-2.29/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/paper.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Richard W Morris</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    R code
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="RCT-in-schools.html">RCT in schools</a>
    </li>
    <li>
      <a href="Mixture-modelling.html">Mixture modelling</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">RCT in schools</h1>

</div>


<p>I recently applied for a senior statistical consultant position. The
interview process included a practical component which I fumbled in the
endzone. But it got me interested in causal inference with nonrandomized
data.</p>
<p>Causal inference, <em>aka</em> estimating the average treatment
effect (ATE, or <span class="math inline">\(\tau\)</span>), usually
requires random allocation to a treatment and a control group but not
always. The critical reference here is <a
href="https://scholar.google.com/scholar_lookup?hl=en&amp;volume=66&amp;publication_year=1974&amp;pages=688&amp;journal=Journal+of+Educational+Psychology&amp;author=D.+B.+Rubin&amp;title=“Estimating+Causal+Effects+of+Treatments+in+Randomized+and+Nonrandomized+Studies%2C”&amp;doi=10.1037%2Fh0037350">Rubin,
1974</a>, which is such a fundamental reference in this field that the
practical component I fumbled was essentially a test of whether I had
read Rubin and the modern representations of it (e.g., <a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5898524/">Twisk et al
2018</a>; <a
href="https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-018-3108-3">Clifton
&amp; Clifton 2019</a>; <a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6290914/">O’Connell
et al 2017</a>).</p>
<p><br></p>
<div id="rubin-1974" class="section level1">
<h1>Rubin 1974</h1>
<p>Randomization guarantees that any pre-existing differences between
groups are non-systematic, so the estimand <span
class="math inline">\(\tau\)</span> will be <strong>unbiased</strong> -
but potentially not very precise. Note that <em>unbiased</em> does not
require random sampling from a broader population or any other
assumption here (although generalizability might). And it only
guarantees <em>unbiased</em> in the long run, as a hidden confound could
(randomly) occur in any particular random draw.</p>
<p>While randomization guarantees that pre-existing differences are not
systematic, it is not the only way to <em>deal</em> with pre-existing
differences. We can also <strong>match</strong> groups on the important
causal variables to remove (or reduce) pre-existing differences. This is
different from randomization which can leave pre-existing differences
between groups, because it removes/reduces the differences and so will
also produce more precise causal estimates. In practice we rarely have a
“complete &amp; true” causal model to guarantee we have removed the
pre-existing differences, and so is the reason we rely on randomization
as it does not depend upon our prior knowledge.</p>
<p>We can match groups by selecting matched pairs, or we can also add
the pre-existing variable as a covariate to our model (e.g., ANCOVA).
The ANCOVA determines the effect of each variable holding the other
covariates at the same value (e.g., 0). When these other covariates
represent pre-existing differences it is similar (but not equivalent?)
to matching.</p>
<p>In nonrandomized designs we do not always want to match pre-existing
differences (!!). In <strong>Lord’s Paradox</strong>, a dietician wanted
to know how the school cafeteria diet affected weight gain among boys
and girls. Pre and post weights were measured at the beginning and end
of term respectively. There were pre-existing differences since boys
were generally heavier than girls at the beginning of term, and there
were also post differences as these pre differences tended to carry over
(plus any differential diet effects). Adding the initial pre weights as
a covariate (i.e., ANCOVA on post weights) will determine whether the
post weights differed between boys and girls with identical frequency
distributions of initial weight. However when equating boys and girls by
the same pre weight, one is selecting relatively light boys and
relatively heavy girls. Under <em>regression-to-the-mean</em>, those
light boys will end up gaining more weight and the heavy girls will end
up gaining less weight, even without any impact of dining hall diet.
Thus when groups are equated, one group appears to be affected by diet
more than the other, using either post weights or weight gain (post -
pre). The important point to draw is that equating for pre-existing
differences is not always pertinent to the research question and can
also produce biased estimates (by introducing RTM effects).</p>
<p><a href="https://en.wikipedia.org/wiki/Lord&#39;s_paradox">Pearl
(2016)</a> provides a succinct explanation for the different estimates
produced by covariate adjustment of pre scores, according to the
difference between direct and total effects. In a causal model of Lords
Paradox, initial weight is a mediating variable of the effect of sex on
final weight (i.e., Sex -&gt; Initial Weight -&gt; Final Weight). Thus
including initial weight as a covariate estimates the <em>direct
effect</em> of sex on final weight (or weight gain). Conversely,
omitting initial weight then estimates the <em>total effect</em> of sex
on final weight. The appropriate estimate (and therefore estimand)
depends on the research question at hand.</p>
<p>In general, the <span class="math inline">\(\tau\)</span> in an RCT
can be estimated from post or gain scores, as pre-existing differences
are random (as are RTM effects) and therefore the estimate is unbiased,
however including pre scores as a covariate in an ANCOVA is always more
efficient (Rubin, 1974). On the other hand, when <a
href="http://www.jerrydallal.com/lhsp/prepost.htm">assignment is not
random and experimental groups are defined by a variable that is
relevant to the change in measurement, then gain scores without
covariate adjustment for pre score differences usually produce a more
relevant estimand</a>.</p>
<p>Below we explore these issues in the context of an educational
program evaluation.</p>
<p><br></p>
<p><br></p>
</div>
<div id="practical-exam" class="section level1">
<h1>Practical exam</h1>
<p>You have been asked to evaluate a numeracy program that is provided
to Year 3 students in NSW government schools. You are required to:</p>
<ul>
<li>interpret the results from an analysis on the short-term impact of
the program; and</li>
<li>evaluate the long-term impact of the program.</li>
</ul>
<div id="about-the-program" class="section level2">
<h2>About the program</h2>
<p>The program was designed to improve numeracy proficiency and is a
whole-of-school intervention. All Year 3 students in participating
schools participate in the program at the beginning of the school year
in 2018.</p>
<p><br></p>
</div>
<div id="task-1-interpretation-of-results-from-an-analysis"
class="section level2">
<h2>Task 1 Interpretation of results from an analysis</h2>
<p>You undertake a statistical analysis of the impact of the program
using a Hierarchical Linear Modelling (HLM) technique. The model has a
two-level structure with students (level-1) nested within schools
(level-2). Table 1 (on page 2) shows the output from the HLM analysis,
using the student data from 2018 to 2019.</p>
<p>The dependent variable in the model was Year 4 Numeracy results. The
model included the following school-level variables:</p>
<ul>
<li>a <strong>treatment</strong> indicator (a binary variable taking the
value 1 when a school was using the program and 0 otherwise);</li>
<li>a <strong>SES</strong> variable (a continuous measure of school
socio-economic status; higher value indicates higher socio-economic
status); and</li>
<li>a <strong>relative school growth</strong> variable (a school growth
score for each school was first calculated as the mean Year 4 scores
minus the mean Year 3 scores). The relative school growth variable was
then calculated as the deviation from the grand mean of the school
growth scores).</li>
</ul>
<p>The model also included the following student-level variables:</p>
<ul>
<li><strong>Year 3 Numeracy scores</strong> (a continuous measure of
numeracy performance in Year 3 <strong><em>PRIOR</em></strong> to the
intervention);</li>
<li><strong>First Nations status</strong> (a binary variable taking the
value 1 for First-Nations students and 0 otherwise);</li>
<li><strong>Gender</strong> (a binary variable taking the value 1 when a
student was female and 0 otherwise);</li>
<li><strong>Language background</strong> (a binary variable taking the
value 1 when a student had a language background other than English and
0 otherwise).</li>
</ul>
<p><br></p>
<div
id="table-1-results-of-hlm-analysis-with-year-4-numeracy-score-as-dependent-variable"
class="section level6">
<h6>Table 1: Results of HLM analysis, with Year 4 Numeracy score as
dependent variable</h6>
<table>
<colgroup>
<col width="32%" />
<col width="22%" />
<col width="14%" />
<col width="9%" />
<col width="9%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="right">Coefficient (B)</th>
<th align="right">Std Error</th>
<th align="right">Beta</th>
<th align="right">t</th>
<th align="right">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Constant</td>
<td align="right">92.011</td>
<td align="right">28.832</td>
<td align="right">1.000</td>
<td align="right">3.191</td>
<td align="right">0.001</td>
</tr>
<tr class="even">
<td align="left">Relative school growth</td>
<td align="right">0.884</td>
<td align="right">0.069</td>
<td align="right">0.246</td>
<td align="right">12.757</td>
<td align="right">0.000</td>
</tr>
<tr class="odd">
<td align="left">Year 3 Numeracy</td>
<td align="right">0.741</td>
<td align="right">0.020</td>
<td align="right">0.711</td>
<td align="right">36.756</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">SES</td>
<td align="right">0.065</td>
<td align="right">0.022</td>
<td align="right">0.066</td>
<td align="right">3.003</td>
<td align="right">0.003</td>
</tr>
<tr class="odd">
<td align="left">Gender</td>
<td align="right">5.216</td>
<td align="right">2.703</td>
<td align="right">0.035</td>
<td align="right">1.930</td>
<td align="right">0.054</td>
</tr>
<tr class="even">
<td align="left">First Nations status</td>
<td align="right">-17.001</td>
<td align="right">6.108</td>
<td align="right">-0.053</td>
<td align="right">-2.783</td>
<td align="right">0.005</td>
</tr>
<tr class="odd">
<td align="left">Language Background</td>
<td align="right">0.107</td>
<td align="right">1.649</td>
<td align="right">0.001</td>
<td align="right">0.065</td>
<td align="right">0.948</td>
</tr>
<tr class="even">
<td align="left">Treatment</td>
<td align="right">4.126</td>
<td align="right">3.070</td>
<td align="right">0.256</td>
<td align="right">1.344</td>
<td align="right">0.179</td>
</tr>
</tbody>
</table>
<p><br></p>
</div>
<div id="what-are-the-assumptions-of-the-model-described-above"
class="section level4">
<h4>What are the assumptions of the model described above?</h4>
<pre><code># From Dorothy Bishop https://doi.org/10.1177/25152459241267904
Analyses of intervention effects conducted in schools can be affected by clustering if randomization is conducted at the level of the schools or classrooms because children within a school/classroom are likely to be more similar to each otherthan to children from different schools/classrooms. Standard errors and p-values from a typical regression model depend on the assumption tha tthe residuals in the model across cases are independent from each other. If children within classrooms are more similar to each other, the assumption of independence of residuals is violated, and this will usually reduce the precision of confidence intervals (and hence reduce the statistical significance of effects).  

The key statistic for any analysis that seeks to control for the effects of clusters is the intracluster correlation coefficient (ICC). The ICC is the proportion of total variance in a dependent variable explained by the cluster variable. So an ICC of 0.15 means the 15% of the variance in scores can be explained by the cluster variable. Typically ICCs greater than 0.10 are considered likely to have significant effects on standard errors. 

If so then we could include school as a fixed effect with an HC1 or HC2 adjustment.</code></pre>
<p>The model above represents an ANCOVA analysis of treatment
differences at the post test, where the Year 3 numeracy scores are
included as a pre score covariate in the model (ANCOVA-post). The <span
class="math inline">\(\beta\)</span> estimate for Treatment determines
whether students with the same initial Year 3 numeracy score are
expected to have the same final Year 4 numeracy score (irrespective of
treatment). However this requires that any pre-existing differences
between program schools relative to schools not in the program are due
to random variation and not some systematic variation (random allocation
will remove any systematic pre-existing differences and so the only
differences remaining will be <em>random</em>). Unfortunately we are not
told whether schools were randomly allocated to treatment groups, and so
we cannot infer this assumption is met.</p>
<p>When random allocation is not present, then intepreting the <span
class="math inline">\(\beta\)</span> for Treatment is not straightfoward
due to potential RTM effects (see below).</p>
<p><em>Nb.</em> The <em>Relative school growth</em> score represents the
relative change for each school, and so adjusts for differences in
growth between each school - that is the clustered changes (among
students) due to school rather than treatment. However it seems likely
to covary with any school-level treatment effect, which is likely to
confound <span class="math inline">\(\tau\)</span> (when included in my
experiments in the analyses below I cannot replicate the results
provided).</p>
<p><br></p>
</div>
<div
id="what-diagnostics-could-you-use-to-check-the-fit-of-the-model-and-whether-the-assumptions-of-the-model-are-valid"
class="section level4">
<h4>What diagnostics could you use to check the fit of the model and
whether the assumptions of the model are valid?</h4>
<p>The most important assumption of the model above is that pre-existing
differences between groups are random (i.e., random allocation to
treatment). There is no way to test this assumption; testing for
significant differences in Year 3 numeracy between the treatment groups
can tell us if pre-existing differances are present but will not help us
determine whether those differences are due to a systematic effect or
random variation (sometimes random allocation results in significant
differences between groups).</p>
<p>Let’s take a look at the data</p>
<table style="width:100%;">
<colgroup>
<col width="10%" />
<col width="11%" />
<col width="11%" />
<col width="9%" />
<col width="7%" />
<col width="14%" />
<col width="20%" />
<col width="5%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">School_ID</th>
<th align="right">Student_ID</th>
<th align="right">Year_level</th>
<th align="right">Numeracy</th>
<th align="right">Gender</th>
<th align="right">First_Nations</th>
<th align="right">Language_background</th>
<th align="right">SES</th>
<th align="left">Treatment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1100067</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">464.3</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1159</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">1100067</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">542.7</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1159</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">1100067</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">531.1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1159</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">1100067</td>
<td align="right">1</td>
<td align="right">6</td>
<td align="right">554.9</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1159</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">1100044</td>
<td align="right">2</td>
<td align="right">3</td>
<td align="right">258.1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1064</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">1100044</td>
<td align="right">2</td>
<td align="right">4</td>
<td align="right">387.2</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1064</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">1100044</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="right">428.1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1064</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">1100044</td>
<td align="right">2</td>
<td align="right">6</td>
<td align="right">542.3</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1064</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">1100044</td>
<td align="right">3</td>
<td align="right">3</td>
<td align="right">352.5</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1064</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">1100044</td>
<td align="right">3</td>
<td align="right">4</td>
<td align="right">456.7</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1064</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">1100044</td>
<td align="right">3</td>
<td align="right">5</td>
<td align="right">533.6</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1064</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">1100044</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="right">617.3</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1064</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">1100045</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="right">449.1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1124</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">1100045</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">509.7</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1124</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">1100045</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">605.8</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1124</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">1100045</td>
<td align="right">4</td>
<td align="right">6</td>
<td align="right">654.8</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1124</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">1100100</td>
<td align="right">5</td>
<td align="right">3</td>
<td align="right">349.2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1163</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">1100100</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">402.9</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1163</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">1100100</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">450.3</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1163</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">1100100</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="right">520.0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1163</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<p>The records are arranged in long format with rows arranged by school,
student and year; and each row representing a student-year
observation.</p>
<p>For Task 1, we will select the data from Years 3 and 4, and rearrange
it into wide format with <code>Pre</code> and <code>Post</code>
scores.</p>
<table>
<colgroup>
<col width="9%" />
<col width="10%" />
<col width="6%" />
<col width="13%" />
<col width="19%" />
<col width="4%" />
<col width="9%" />
<col width="5%" />
<col width="5%" />
<col width="7%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">School_ID</th>
<th align="right">Student_ID</th>
<th align="right">Gender</th>
<th align="right">First_Nations</th>
<th align="right">Language_background</th>
<th align="right">SES</th>
<th align="left">Treatment</th>
<th align="right">Pre</th>
<th align="right">Post</th>
<th align="right">cPre</th>
<th align="right">Change</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1100021</td>
<td align="right">41</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">358.9</td>
<td align="right">507.9</td>
<td align="right">-48.194</td>
<td align="right">148.98</td>
</tr>
<tr class="even">
<td align="left">1100021</td>
<td align="right">59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">467.0</td>
<td align="right">571.2</td>
<td align="right">59.926</td>
<td align="right">104.18</td>
</tr>
<tr class="odd">
<td align="left">1100021</td>
<td align="right">61</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">493.2</td>
<td align="right">529.4</td>
<td align="right">86.146</td>
<td align="right">36.22</td>
</tr>
<tr class="even">
<td align="left">1100021</td>
<td align="right">62</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">361.1</td>
<td align="right">421.1</td>
<td align="right">-45.974</td>
<td align="right">59.96</td>
</tr>
<tr class="odd">
<td align="left">1100021</td>
<td align="right">67</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">413.1</td>
<td align="right">523.9</td>
<td align="right">5.986</td>
<td align="right">110.89</td>
</tr>
<tr class="even">
<td align="left">1100021</td>
<td align="right">68</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">378.6</td>
<td align="right">435.1</td>
<td align="right">-28.514</td>
<td align="right">56.54</td>
</tr>
<tr class="odd">
<td align="left">1100021</td>
<td align="right">69</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">405.7</td>
<td align="right">420.7</td>
<td align="right">-1.334</td>
<td align="right">14.94</td>
</tr>
<tr class="even">
<td align="left">1100021</td>
<td align="right">70</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">359.0</td>
<td align="right">416.7</td>
<td align="right">-48.074</td>
<td align="right">57.72</td>
</tr>
<tr class="odd">
<td align="left">1100021</td>
<td align="right">74</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">429.1</td>
<td align="right">502.5</td>
<td align="right">22.066</td>
<td align="right">73.36</td>
</tr>
<tr class="even">
<td align="left">1100021</td>
<td align="right">76</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">563.5</td>
<td align="right">599.3</td>
<td align="right">156.426</td>
<td align="right">35.76</td>
</tr>
<tr class="odd">
<td align="left">1100021</td>
<td align="right">77</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">375.6</td>
<td align="right">475.1</td>
<td align="right">-31.494</td>
<td align="right">99.48</td>
</tr>
<tr class="even">
<td align="left">1100021</td>
<td align="right">78</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">522.4</td>
<td align="right">616.5</td>
<td align="right">115.366</td>
<td align="right">94.05</td>
</tr>
<tr class="odd">
<td align="left">1100021</td>
<td align="right">81</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">450.0</td>
<td align="right">556.9</td>
<td align="right">42.966</td>
<td align="right">106.84</td>
</tr>
<tr class="even">
<td align="left">1100021</td>
<td align="right">82</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">481.0</td>
<td align="right">540.9</td>
<td align="right">73.906</td>
<td align="right">59.94</td>
</tr>
<tr class="odd">
<td align="left">1100021</td>
<td align="right">85</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">479.7</td>
<td align="right">586.0</td>
<td align="right">72.666</td>
<td align="right">106.24</td>
</tr>
<tr class="even">
<td align="left">1100021</td>
<td align="right">88</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">462.5</td>
<td align="right">473.0</td>
<td align="right">55.466</td>
<td align="right">10.48</td>
</tr>
<tr class="odd">
<td align="left">1100021</td>
<td align="right">91</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">421.6</td>
<td align="right">501.4</td>
<td align="right">14.526</td>
<td align="right">79.75</td>
</tr>
<tr class="even">
<td align="left">1100021</td>
<td align="right">92</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">469.8</td>
<td align="right">599.0</td>
<td align="right">62.766</td>
<td align="right">129.16</td>
</tr>
<tr class="odd">
<td align="left">1100021</td>
<td align="right">94</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">505.5</td>
<td align="right">602.6</td>
<td align="right">98.406</td>
<td align="right">97.10</td>
</tr>
<tr class="even">
<td align="left">1100021</td>
<td align="right">96</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1180</td>
<td align="left">1</td>
<td align="right">459.7</td>
<td align="right">582.8</td>
<td align="right">52.626</td>
<td align="right">123.09</td>
</tr>
</tbody>
</table>
<p>Note that pre-existing numeracy differences between treatment groups
are present:</p>
<pre class="r"><code>df %&gt;%
  filter(Year_level == 3) %&gt;%
  lm(Numeracy ~ 1 + Treatment,
       data = .) %&gt;%
  tidy()</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)    423.       2.20     192.  0       
## 2 Treatment1     -31.2      3.11     -10.0 4.73e-23</code></pre>
<p>The intercept is the mean numeracy level among the non-program
schools in Year 3, while the parameter for the treatment effect
represents a pre-existing deficit in numeracy scores among the program
schools.</p>
<p>Also note the demographic representation of minorities, lower SES
regions and males is higher in the program schools, consistent with
enrolment of schools with poor educational resources into the
program</p>
<pre class="r"><code>df %&gt;%
  filter(Year_level == 3) %&gt;%
  group_by(Treatment) %&gt;%
  summarise(
    Numeracy = mean(Numeracy),
    First_Nations = mean(First_Nations),
    Language = mean(Language_background),
    SES = mean(SES),
    Females = mean(Gender))</code></pre>
<pre><code>## # A tibble: 2 × 6
##   Treatment Numeracy First_Nations Language   SES Females
##   &lt;fct&gt;        &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 0             423.        0.0291    0.181 1042.   0.488
## 2 1             392.        0.0993    0.322 1038.   0.516</code></pre>
<p><br></p>
<p>Other assumptions of HLMs are straightforward to check as a measure
of model fit. For instance the residual distributions in the model
should be normally distributed around zero, which can be checked by
visual inspection.</p>
<pre><code># check whether residuals are normally distributed around 0
plot(fit)  </code></pre>
<p><br></p>
<p>The specification of the random effect distribution should also be
assessed in a HLM. Variance among different levels of the random effects
should be similar. An ANOVA can be used to test for an overall
difference in the squared residuals (<span
class="math inline">\(r_{ij}^2\)</span>) between levels. Or residuals
can be plotted stratified by grouping level, inspecting for similar
spread between levels and no correlations or patterns over levels.</p>
<pre><code># check there is no heteroscedasticity among different levels of the random effects 
plot(fit, as.factor(school) ~ resid(., scaled=TRUE), 
     abline=0, pch=16,
     xlab=&quot;Standardised residuals&quot;, 
     ylab=&quot;School&quot;)</code></pre>
<p><br></p>
<p><br></p>
</div>
<div
id="what-are-some-limitations-andor-caveats-of-the-analysis-how-could-the-model-be-improved-to-overcome-some-of-these-limitations"
class="section level4">
<h4>What are some limitations and/or caveats of the analysis? How could
the model be improved to overcome some of these limitations?</h4>
<p>The coefficient for <em>Treatment</em> represents the average
treatment effect (<span class="math inline">\(\tau\)</span>) on post
(Year 4) scores, but <strong>if and only if</strong> the assignment to
treatment was random (i.e., RCT). However randomization is often
considered impractical when evaluating school programs in disadvantaged
communities. Instead comparisons are often made with schools not in the
program, and who are also not (as) disadvantaged/deprived. Consequently
the post scores will tend to regress towards their respective population
means and spuriously cause the program to appear ineffective or even
harmful. An ANCOVA assessing post score differences <strong>without
randomization</strong> will be misleading, even after adjusting for
pre-existing differences, due to the potential presence of such
regression-to-the-mean effects.</p>
<p>In such cases it is still possible to determine whether program
schools <em>gain</em> more numeracy on average than schools not in the
program by the estimand of a change/gain score analysis. It is important
to note that a change/gain score analysis answers a distinctly different
question from the post score ANCOVA, namely whether program schools gain
the same amount of numeracy on average as the other schools (by contrast
the ANCOVA assesses the difference in post means after equating for pre
differences). According to <a
href="https://www.researchgate.net/publication/10854793_Making_friends_with_your_data_Improving_how_statistics_are_conducted_and_reported">Wright
(2003)</a>, the only procedure that is always correct in this situation
is a scatterplot comparing the scores at time 2 with those at time 1 for
the different groups.</p>
<pre class="r"><code>p1 &lt;- pdf %&gt;%
  ggplot(aes(x = Pre, y = Post, color = Treatment)) +
    geom_point(alpha = 0.6) +
    geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se=F) +
    scale_color_manual(values = c(blues9[5], blues9[9])) +# c(&quot;#377EB8&quot;, &quot;#E41A1C&quot;)
    labs(subtitle = &quot;Year 4 numeracy (dark blue = program)&quot;,
         y = &quot;&quot;, x = &quot;Year 3 numeracy&quot;)

p2 &lt;- pdf %&gt;%
  ggplot(aes(x = Pre, y = Change, color = Treatment)) +
    geom_point(alpha = 0.6) +
    geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se=F) +
    scale_color_manual(values = c(blues9[5], blues9[9])) +# c(&quot;#377EB8&quot;, &quot;#E41A1C&quot;)
    labs(subtitle = &quot;Change scores&quot;,
         y = &quot;&quot;, x = &quot;Year 3 numeracy&quot;)

p1 + p2 + plot_layout(guides = &#39;collect&#39;) &amp;
  theme_minimal() + 
  theme(legend.position = &quot;bottom&quot;,
        axis.title.x = element_text(size = 10, color = &quot;grey30&quot;))</code></pre>
<p><img src="RCT-in-schools_files/figure-html/rct_prevpost_1-1.png" width="672" /></p>
<p>The distinct slopes for each treatment group are shown in dark blue
(program) and light blue (non-program). The vertical distance between
these slopes in each panel indicate the treatment effect. Where the
slopes are not parallel indicates the program may have a
<em>heterogenous treatment effect</em> across Year 3 numeracy levels. In
addition, the negative slope in the right panel shows scores from all
schools tend to regress to the mean - high Year 3 scores produce smaller
gains (change scores) than low Year 3 scores. So the difference in slope
in the right panel shows the program improves high scoring students more
than low scoring students, thus reducing regression-to-the-mean effects
in these schools relative to other schools.</p>
<p><br></p>
<p>Below we compare the estimates from a post score ANCOVA and a change
score analysis.</p>
<p>An ANCOVA on the post scores produces an estimate which is similar to
that provided in the question material above</p>
<pre class="r"><code>fit.1 &lt;- lmer(Post ~ 1 + Treatment + cPre + (1|School_ID),
              data = as.data.frame(pdf)) 

tidy(fit.1, effects = &quot;fixed&quot;) %&gt;%
  filter(term == &quot;Treatment1&quot;) # 3.29 (4.88) vs 4.126 (3.070)</code></pre>
<pre><code>## # A tibble: 1 × 7
##   effect term       estimate std.error statistic    df p.value
##   &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 fixed  Treatment1     3.29      4.88     0.675  47.4   0.503</code></pre>
<p>The estimated effect of the program on Year 4 numeracy scores from
the ANCOVA equating Year 3 numeracy level was <strong>3.29
(±4.88)</strong> points (compare to Table 1 above).</p>
<p>However without randomization, equating for Year 3 numeracy is likely
to make the program look ineffective due to regression-to-the-mean in
each respective group. Instead if we estimate the <em>relative gain</em>
of program schools over other schools then we may still determine
whether any benefit accrued to the program schools after treatment.</p>
<pre class="r"><code># Adding covariates wiil improve the precision but not change the estimand
pdf %&gt;%
  lmer(
    Post ~ 1 + Treatment + cPre + First_Nations + SES + Gender + Language_background +
      (1|School_ID),
    data = .) %&gt;%
  tidy(effects = &quot;fixed&quot;) %&gt;%
  filter(term == &quot;Treatment1&quot;) # 4.07 (4.47) vs 4.126 (3.070)</code></pre>
<p><br></p>
<p><br></p>
<p>The simplest change score analysis is done with a t-test/OLS. The
unadjusted estimate is the difference between the mean differences, and
shows a much more profound improvement among program schools. This
produces an unbiased estimate however it does not account for the
expected dependency between students from the same school, and so does
not control the type-I error rate at the nominal level (i.e., the
p-value is too low)</p>
<pre class="r"><code>pdf %&gt;%
  mutate(Treatment = fct_rev(Treatment)) %&gt;% # recode so gain is over non-program schools
  t.test(Change ~ Treatment, data = .) %&gt;% 
  tidy() %&gt;%
  select(`0` = estimate2, `1` = estimate1, estimate:conf.high) # 9.50118, p = .000000862</code></pre>
<pre><code>## # A tibble: 1 × 8
##     `0`   `1` estimate statistic     p.value parameter conf.low conf.high
##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1  80.8  90.3     9.50      4.94 0.000000862     1920.     5.73      13.3</code></pre>
<p>The estimate <strong>9.501</strong> from the <em>t</em>-test is an
unbiased estimate, however the error term does not respect the
dependency within schools in the data.</p>
<p>To account for the dependency between students from the same school,
an error term for each school can be added (i.e., random intercept) in
an RM ANOVA. However the estimate will be biased using an RM ANOVA in
this unbalanced dataset</p>
<pre class="r"><code>aov(Change ~ 1 + Treatment + Error(School_ID), # add cPre to reduce bias?
    data = pdf) -&gt; aov.change.school 

aov.change.school %&gt;% 
  emmeans::emmeans(&quot;Treatment&quot;) %&gt;%
  as.data.frame() %&gt;%
  select(Treatment, emmean) %&gt;%
  spread(Treatment, emmean) %&gt;%
  mutate(estimate = `1` - `0`) # 10.52004, p = .00000119</code></pre>
<pre><code>## Note: re-fitting model with sum-to-zero contrasts</code></pre>
<pre><code>##       0     1 estimate
## 1 80.77 90.28    9.501</code></pre>
<p><br></p>
<p>To account for the dependency in an unbalanced dataset and so produce
an unbiased estimate, we need a linear mixed model with a random
intercept for each school</p>
<pre class="r"><code>fit.2 &lt;- lmer(Change ~ 1 + Treatment + (1|School_ID),
     data = as.data.frame(pdf))

fit.2 %&gt;%
  tidy(effects = &quot;fixed&quot;) %&gt;%
  filter(term == &quot;Treatment1&quot;) # 9.88 (3.82), p = .0128</code></pre>
<pre><code>## # A tibble: 1 × 7
##   effect term       estimate std.error statistic    df p.value
##   &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 fixed  Treatment1     9.88      3.82      2.59  48.1  0.0128</code></pre>
<p>The estimate from the LMM of <strong>9.88 (3.82)</strong> is similar
(but not equivalent) to the t-test result. Nevertheless, the degrees of
freedom (and therefore the error) now better reflects the dependency
among students from the same school.</p>
<p>When considering this result, the question arises whether we should
adjust for pre-existing differences in Year 3 numeracy (and/or other
pre-existing differences in demographics). As far as I can tell, in the
context of a pseudoexperimental design such as this, the experts
consensus appears to be that covariates for pre-existing differences
should not be included in a change score analysis, as these real
differences are likely to mask the total effects of the program (Twisk
et al, 2018; Dallal 2020). According to Pearl (2016), the decision rests
on where the Year 3 numeracy levels stand with respect to the program
allocation in a causal DAG. If the schools were allocated to the program
<em>because</em> of their low numeracy levels then equating for numeracy
levels and including them as a covariate is warranted as we might be
interested in the direct effect of the program. However if low numeracy
levels are merely a mediator of the program effect on numeracy then we
should not adjust and so estimate the total effect of the program. We
aren’t told in this example how allocation to the program was performed,
so the only reasonable justification I can provide is that we should be
interested in how much the program improved numeracy among the
disadvantaged schools, rather than whether the program will improve
numeracy among all schools on average.</p>
<pre class="r"><code># The effect of equating Year 3 numeracy scores on predicted gain
p1 &lt;- ggpredict(fit.2, terms = &quot;Treatment&quot;) %&gt;% plot() +
  coord_cartesian(ylim = c(75, 95)) +
  labs(subtitle = &quot;without equating Year 3 numeracy&quot;)

# Nb. Adding cPre turns this model into the post scores ANCOVA
fit.2b &lt;- lmer(Change ~ 1 + Treatment + cPre + (1|School_ID),
     data = as.data.frame(pdf)) # 3.29 (4.88)

p2 &lt;- ggpredict(fit.2b, terms = &quot;Treatment&quot;) %&gt;% plot() +
  coord_cartesian(ylim = c(75, 95)) +
  labs(subtitle = &quot;after equating Year 3 numeracy&quot;)

p1 + p2</code></pre>
<p><br></p>
<p><br></p>
<p>We can also estimate gain in a mixed 2 x (2) ANOVA with treatment and
time (Pre, Post) as factors, where the interaction term
(<code>Time:Treatment</code>) represents the average gain of the program
schools over the other schools.</p>
<p>In a RM ANOVA, <code>Time</code> is nested within <code>School</code>
and this must be entered into the error term. Note the result matches
the RM ANOVA on the change scores (which was biased because of
unbalanced cells and is biased here too)</p>
<pre class="r"><code>lpdf &lt;- df %&gt;%
  filter(Year_level %in% 3:4) %&gt;%
  left_join(
    filter(df, Year_level == 3) %&gt;%
      select(School_ID, Student_ID, Pre = Numeracy) %&gt;%
      mutate(cPre = c(scale(Pre, scale=F))),
    by = join_by(School_ID, Student_ID)
  ) %&gt;%
  mutate(Time = Year_level - 3) 

aov.interact.school &lt;- aov(Numeracy ~ Treatment*Time + Error(School_ID/Time),
                           data = lpdf) 

emmeans::emmeans(aov.interact.school,  ~ Time|Treatment) %&gt;%
  as.data.frame() %&gt;%
  select(Time, Treatment, emmean) %&gt;%
  spread(Time, emmean) %&gt;%
  mutate(delta = `1` - `0`) %&gt;%
  select(Treatment, delta) %&gt;%
  spread(Treatment, delta) %&gt;%
  mutate(estimate = `1` - `0`) # 10.52004, p = .0335</code></pre>
<pre><code>## Note: re-fitting model with sum-to-zero contrasts</code></pre>
<pre><code>## Warning in (mth$objs[[1]])(object, trms, xlev, grid, ...): Some predictors are correlated with
## the intercept - results may be very biased</code></pre>
<pre><code>##       0     1 estimate
## 1 80.76 90.26    9.501</code></pre>
<pre class="r"><code># Nb. If we don&#39;t add Time as a nested variable here, we get the same result as 
# the t-test on the change scores above (est. = 9.501182)  </code></pre>
<p><br></p>
<p>Testing the same model in a linear mixed model setting, where the
interaction term (<code>Time:Treatment</code>) represents the gain of
program schools over other schools</p>
<pre class="r"><code>fit.3 &lt;- df %&gt;%
  filter(Year_level %in% 3:4) %&gt;%
  mutate(Time = Year_level - 3) %&gt;%
  as.data.frame() %&gt;%
  lmer(Numeracy ~ 1 + Time*Treatment + (1|School_ID/Time),
       data = .) 

fit.3 %&gt;%
  tidy(effects = &quot;fixed&quot;) %&gt;%
  filter(term == &quot;Time:Treatment&quot;) # 9.51 (4.01), p = .0213</code></pre>
<pre><code>## # A tibble: 0 × 7
## # ℹ 7 variables: effect &lt;chr&gt;, term &lt;chr&gt;, estimate &lt;dbl&gt;, std.error &lt;dbl&gt;, statistic &lt;dbl&gt;,
## #   df &lt;dbl&gt;, p.value &lt;dbl&gt;</code></pre>
<pre class="r"><code># Nb. Adding Pre as a covariate on the RHS is invalid since Pre is already 
# included on the LHS in the first level of Time
fit.3b &lt;- df %&gt;%
  filter(Year_level %in% 3:4) %&gt;%
  mutate(Time = Year_level - 3) %&gt;%
  left_join(
    filter(df, Year_level == 3) %&gt;%
      select(School_ID, Student_ID, Pre = Numeracy) %&gt;%
      mutate(cPre = c(scale(Pre, scale=F))),
    by = join_by(School_ID, Student_ID)
  ) %&gt;%
  as.data.frame() %&gt;%
  lmer(Numeracy ~ 1 + Time*Treatment + cPre + (1|School_ID/Time),
       data = .) # 9.89 (3.82), p = .0127

p1 &lt;- ggpredict(fit.3, terms = c(&quot;Time&quot;, &quot;Treatment&quot;)) %&gt;% plot() +
  coord_cartesian(ylim = c(380, 520)) +
  labs(subtitle = &quot;without equating Year 3 numeracy&quot;)

p2 &lt;- ggpredict(fit.3b, terms = c(&quot;Time&quot;, &quot;Treatment&quot;)) %&gt;% plot() +
  coord_cartesian(ylim = c(380, 520)) +
  labs(subtitle = &quot;after equating Year 3 numeracy&quot;)

p1 + p2 + plot_layout(guides = &#39;collect&#39;) &amp;
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p>The LMM with nested groups for school and time produces the best
unbiased estimate of <strong>9.51 (±4.01)</strong> with the correct
error-term.</p>
<p>I’m not sure why this produces a different estimate from the LMM on
change scores above (to be considered later?), but this estimate is
closer to the t-test estimate and so I’m guessing this model produces
the most unbiased estimate with the correct error rate.</p>
<p><br></p>
<p>We can also relax the parallel slopes assumption (aka
sphericity/compound symmetry), but unless there is a good reason to
believe different schools will show different gains over time then this
might not be justified…</p>
<pre class="r"><code>fit.4 &lt;- df %&gt;%
  filter(Year_level %in% 3:4) %&gt;%
  mutate(Time = Year_level - 3) %&gt;%
  as.data.frame() %&gt;%
  lmer(Numeracy ~ 1 + Time*Treatment + (1 + Time|School_ID/Time),
       data = .) </code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<pre class="r"><code>fit.4 %&gt;% 
  tidy(effects = &quot;fixed&quot;) %&gt;%
  filter(term == &quot;Time:Treatment&quot;) # 9.98 (4.48), p = .0277</code></pre>
<pre><code>## # A tibble: 0 × 7
## # ℹ 7 variables: effect &lt;chr&gt;, term &lt;chr&gt;, estimate &lt;dbl&gt;, std.error &lt;dbl&gt;, statistic &lt;dbl&gt;,
## #   df &lt;dbl&gt;, p.value &lt;dbl&gt;</code></pre>
<p>This gives us a similar estimate (with less precision) to the LMM
with change scores above.</p>
<p>To answer the question, the post scores ANCOVA relies on random
allocation to the program in order to provide an unbiased average
treatment effect (<span class="math inline">\(\tau\)</span>). Without
random allocation, we cannot assume the pre-existing differences are
random and not real. In the presence of real pre-existing differences,
we can still determine whether the program improved numeracy among the
disadvantaged schools by defining a model of gain. In this unbalanced
dataset, the best (unbiased) model is a LMM with nested effects of
school and time.</p>
<p>Wihtout random allocation to groups, any estimated differences may be
due to pre-existing differences between the program schools and the
other schools rather than the program, or even due to an interaction
between the program and the schools (i.e., if the program only works in
the <em>kind</em> of schools in the program); and so the estimand does
not represent the <em>average</em> treatment effect (<span
class="math inline">\(\tau\)</span>). Nevertheless, an estimate of the
difference in slope may be an informative of the success of the program
(i.e., in <em>those</em> schools), especially when we have no reason to
expect program schools to gain on other schools without treatment.</p>
<p><br></p>
<p><br></p>
</div>
<div
id="what-can-you-conclude-from-this-model-about-the-short-term-impact-of-the-program-on-student-numeracy-proficiency-after-one-year"
class="section level4">
<h4>What can you conclude from this model about the short-term impact of
the program on student numeracy proficiency after one year?</h4>
<p>If allocation to the program was not random AND schools were placed
in the program <em>because</em> they had lower numeracy levels, then the
post score ANCOVA treatment estimate above may include RTM effects and
produce a biased estimate. Accordingly, low scoring program schools are
equated with high scoring non-program schools, so the best performing
program schools are likely to improve least in numeracy while the worst
performing non-program schools are likely to improve the most. This
would result in underestimating the effect of the program, and making
interpretation of the non-signifcant positive effect (<span
class="math inline">\(\beta = 4.126\)</span>, <span
class="math inline">\(p &gt; .05\)</span>) difficult.</p>
<p>If allocation to the program was random, then the average treatment
effect of the program on Year 4 numeracy is small (4.126) but everything
from moderate positive effects (10.1432) to small negative effects
(-1.8192) are consistent with the data, including a zero effect (<span
class="math inline">\(p &gt; .05\)</span>).</p>
<p>Regarding other practical considerations: We don’t know if the effect
of treatment is interacting with any of the other predictor variables
(e.g., Year 3 numeracy, relative school growth or SES). Adding
interaction terms would help determine the presence of varying effects
of treatment and so better understand the conditions under which the
treatment works, which may help when planning wider implementation or
policy.</p>
<p><br></p>
<p><br></p>
</div>
</div>
<div id="task-2-investigation-of-long-term-impact-of-treatment"
class="section level2">
<h2>Task 2 Investigation of long-term impact of treatment</h2>
<p>You are going to undertake a statistical analysis to evaluate the
long-term impact of the program. To do this, the numeracy performance of
the student sample from Task 1 was tracked to Year 6. The annual
numeracy results from Year 3 to Year 6 for both treatment and control
groups are given in the data file “Year 3-6 numeracy.csv”. The data is
in long format meaning that a student has multiple rows of records.</p>
<p>Same demographic variables as in Task 1 (except relative school
growth) are recorded in the data file. Student IDs, School IDs and the
numeracy scores from Year 3 to Year 6 are also included. The
“Year_level” variable indicates which Year level the record comes
from.</p>
<p><br></p>
<div id="estimate-the-long-term-effect-of-treatment"
class="section level4">
<h4>Estimate the long-term effect of treatment</h4>
<p>The suggested solution to a similar design, albiet in the context of
an RCT, is offered by <a
href="https://doi.org/10.1186%2Fs13063-020-4114-9" target="_blank">Bell
&amp; Rabe, 2020</a>. Based on simulations of cluster randomized trial
data where the outcome was continuous and measured at baseline and three
post-intervention time points (as we have here) they suggested the
following model:</p>
<pre class="r"><code>library(nlme)
library(contrast)

# “treat” and “time” are factors with levels (0,1) and (0,1,2,3) respectively.

Model1 &lt;- lme(y ~ treat*time, 
    random = ~ 1 | cluster_id,
    weights = varIdent(form = ~ 1 | time),
    correlation = corSymm(form = ~ 1 | cluster_id/subject_id),
    data = dataset, control = lmeControl(maxIter=10000, msMaxIter = 10000))
summary(Model1)

# Note that this contrast does not use the Kenward-Roger correction for degrees
# of freedom which is not implemented in nlme.

contrast(Model1, list(time = &#39;3&#39;, treat = &#39;1&#39;), list(time = &#39;3&#39;, treat = &#39;0&#39;))</code></pre>
<p>This mixed model for repeated measures (MMRM) uses an unstructured
time and covariance structure. Unstructured time means that time is
modeled categorically, rather than continuously as a linear or
polynomial function, and allows for an arbitrary trajectory over time.
While the continuous time models may use fewer degrees of freedom and
may, therefore, be more powerful, it can be difficult to anticipate the
outcome’s time trajectory in advance. Since clinical trials often
require a pre-specified analysis plan, unstructured time can be
appealing. In the context of randomized controlled trials, fixed effects
of time, treatment and their interaction are included in the MMRM model.
The interaction term accommodates different patterns of change over time
between the arms. Baseline values of the outcome are sometimes included.
Maximum-likelihood-based mixed models provide unbiased estimation for
data that are MCAR or MAR, as long as the model is not misspecified. All
outcome data are used, regardless of whether an individual has complete
data or not, making these models consistent with an intention-to-treat
analysis.</p>
<p>Cluster randomized trials with longitudinally measured outcomes have
two sources of non-independence: the cluster and the repeated measures
over time. Linear mixed-effects models are one option for handling the
non-independence of measurements over time. In the mixed-model context,
one may use a random-coefficients model, using random effects for a
subject’s intercept and sometimes slope. Alternatively, one may use
covariance pattern models, where the covariance between repeated
measures on the same subject is modeled explicitly from the residual
effects. Some commonly used covariance structures, available in
statistical software, include compound symmetric, autoregressive, or
unstructured. A compound symmetric structure assumes that any two
measurements on the same individual have the same covariance, regardless
of timing. An autoregressive structure assumes that measurements’
correlation drops over time exponentially. Unstructured covariance makes
no assumptions about the correlation between measurements, thereby
rendering misspecification not a problem; however, it can require that a
large number of parameters must be estimated. However, many cluster
trials have a fairly small number of assessments on each subject, as we
have here.</p>
<p>This model is easily extended to include more than two arms, the
baseline value of the outcome variable as a covariate (instead of in the
outcome vector as shown here), and/or a baseline by treatment arm
interaction.</p>
<p>Our interest was the long-term effect of the program, so under an RCT
assumption we could ignore pre-existing differences at baseline (Time 0)
and focus on the difference at the fourth time point between the
treatment arms, which can be estimated using a contrast within the
model.</p>
<pre class="r"><code>library(nlme)
library(ggeffects)

fit.9 &lt;- df %&gt;% 
  mutate(Time = as.factor(Year_level - 3),
         Student_ID = as.factor(Student_ID)) %&gt;%
  as.data.frame() %&gt;%
  lme(Numeracy ~ 1 + Time*Treatment,
       random = ~1 | School_ID,
       weights = varIdent(form = ~ 1 | Time),
       correlation = corSymm(form = ~ 1 | School_ID/Student_ID),
       data = ., control = lmeControl(maxIter=10000, msMaxIter = 10000))

summary(fit.9)</code></pre>
<pre><code>## Linear mixed-effects model fit by REML
##   Data: . 
##     AIC   BIC logLik
##   78891 79023 -39427
## 
## Random effects:
##  Formula: ~1 | School_ID
##         (Intercept) Residual
## StdDev:       25.94    62.67
## 
## Correlation Structure: General
##  Formula: ~1 | School_ID/Student_ID 
##  Parameter estimate(s):
##  Correlation: 
##   1     2     3    
## 2 0.771            
## 3 0.746 0.805      
## 4 0.681 0.726 0.795
## Variance function:
##  Structure: Different standard deviations per stratum
##  Formula: ~1 | Time 
##  Parameter estimates:
##      0      1      2      3 
## 1.0000 0.9911 0.9408 0.8414 
## Fixed effects:  Numeracy ~ 1 + Time * Treatment 
##                  Value Std.Error   DF t-value p-value
## (Intercept)      421.4     5.672 7661   74.30  0.0000
## Time1             80.8     1.363 7661   59.26  0.0000
## Time2            129.3     1.403 7661   92.18  0.0000
## Time3            173.6     1.514 7661  114.67  0.0000
## Treatment1       -29.7     7.951   47   -3.73  0.0005
## Time1:Treatment1   9.5     1.925 7661    4.94  0.0000
## Time2:Treatment1  19.8     1.981 7661    9.99  0.0000
## Time3:Treatment1  24.8     2.138 7661   11.62  0.0000
##  Correlation: 
##                  (Intr) Time1  Time2  Time3  Trtmn1 Tm1:T1 Tm2:T1
## Time1            -0.125                                          
## Time2            -0.153  0.608                                   
## Time3            -0.203  0.531  0.681                            
## Treatment1       -0.713  0.089  0.109  0.145                     
## Time1:Treatment1  0.088 -0.708 -0.431 -0.376 -0.126              
## Time2:Treatment1  0.108 -0.431 -0.708 -0.482 -0.154  0.608       
## Time3:Treatment1  0.144 -0.376 -0.482 -0.708 -0.204  0.531  0.681
## 
## Standardized Within-Group Residuals:
##     Min      Q1     Med      Q3     Max 
## -3.6256 -0.7022 -0.0450  0.6206  3.4371 
## 
## Number of Observations: 7716
## Number of Groups: 49</code></pre>
<pre class="r"><code>ggpredict(fit.9, terms = c(&quot;Time&quot;, &quot;Treatment&quot;)) %&gt;% plot()</code></pre>
<p><img src="RCT-in-schools_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>hypothesis_test(fit.9, c(&quot;Time [3]&quot;, &quot;Treatment&quot;))</code></pre>
<pre><code>## # Pairwise comparisons
## 
## Time | Treatment | Contrast |        95% CI |     p
## ---------------------------------------------------
## 3-3  |       0-1 |     4.83 | -10.86, 20.52 | 0.539</code></pre>
<p>First plot the scatterplot between every post year and the pre
year.</p>
<pre class="r"><code>p1 &lt;- df %&gt;%
  select(School_ID, Student_ID, Year_level, Numeracy, Treatment) %&gt;%
  spread(Year_level, Numeracy) %&gt;%
  ggplot(aes(x = `3`, color = Treatment)) +
    geom_point(aes(y = `4`), alpha = 0.2) +
    geom_smooth(aes(y = `4`), formula = &#39;y ~ x&#39;, method = &quot;lm&quot;, se = F) +
    labs(subtitle = &quot;Year 4&quot;)
    

p2 &lt;- df %&gt;%
  select(School_ID, Student_ID, Year_level, Numeracy, Treatment) %&gt;%
  spread(Year_level, Numeracy) %&gt;%
  ggplot(aes(x = `3`, color = Treatment)) +
    geom_point(aes(y = `5`), alpha = 0.2) +
    geom_smooth(aes(y = `5`), formula = &#39;y ~ x&#39;, method = &quot;lm&quot;, se = F) +
  labs(subtitle = &quot;Year 5&quot;)
    

p3 &lt;- df %&gt;%
  select(School_ID, Student_ID, Year_level, Numeracy, Treatment) %&gt;%
  spread(Year_level, Numeracy) %&gt;%
  ggplot(aes(x = `3`, color = Treatment)) +
    geom_point(aes(y = `6`), alpha = 0.2) +
    geom_smooth(aes(y = `6`), formula = &#39;y ~ x&#39;, method = &quot;lm&quot;, se = F) +
    labs(subtitle = &quot;Year 6&quot;)
    

p1 + p2 + p3 + plot_layout(guides = &#39;collect&#39;) &amp; 
  coord_cartesian(ylim = c(300, 750)) &amp;
  scale_color_manual(values = c(blues9[5], blues9[9])) &amp;# c(&quot;#377EB8&quot;, &quot;#E41A1C&quot;)
  theme_minimal() &amp; 
  labs(y = &quot;&quot;, x = &quot;Year 3 (pre numeracy)&quot;) &amp;
  theme(legend.position = &quot;bottom&quot;,
        axis.title.x = element_text(size = 10, color = &quot;grey30&quot;))</code></pre>
<p><img src="RCT-in-schools_files/figure-html/rct_prevpost_2-1.png" width="864" /></p>
<p>The distinct slopes for each treatment group are shown in dark blue
(treatment) and light blue (control). The vertical distance between
these slopes in each panel indicate the treatment effect for each year.
Where the slopes are not parallel indicates the treatment may have a
heterogenous effect across Year 3 numeracy levels.</p>
<p><br></p>
<p>Assuming a linear effect of time, we can test a similar interaction
model as suggested in Task 1, which will determine the (average) gain
per year among program schools relative to other schools, over Years 3
to 6.</p>
<pre class="r"><code>fit.5 &lt;- lmer(Numeracy ~ Treatment*Year_level + (1|School_ID/Year_level),
              data = df)

tidy(fit.5, effects = &quot;fixed&quot;) %&gt;%
  filter(term == &quot;Treatment1:Year_level&quot;) # 8.38 (1.85)</code></pre>
<pre><code>## # A tibble: 1 × 7
##   effect term                  estimate std.error statistic    df   p.value
##   &lt;chr&gt;  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
## 1 fixed  Treatment1:Year_level     8.38      1.85      4.53  146. 0.0000121</code></pre>
<pre class="r"><code># adding covariates does not change this estimate 

lmer(Numeracy ~ Treatment*Year_level + SES + Gender + First_Nations + 
       Language_background + (1|School_ID/Year_level),
     data = df) %&gt;%
  tidy(effects = &quot;fixed&quot;) %&gt;%
  filter(term == &quot;Treatment1:Year_level&quot;) # 8.38 (1.85)</code></pre>
<p>The interaction between <code>Treatment</code> and
<code>Year_level</code> indicates the long-term impact of the program
over other schools. The average relative gain of program schools from
Year 3 to Year 6 is 8.5 points over other schools.</p>
<p><br></p>
<p>To determine the long-term effect of the program over any short-term
effect, we can omit Year 3 from the interaction and estimate the
relative gain from Year 4. Including the Year 3 scores as a (centered)
covariate will equate pre-existing differences in this model.</p>
<pre class="r"><code>left_join(df, 
          filter(df, Year_level==3) %&gt;% 
            select(School_ID, Student_ID, Pre = Numeracy) %&gt;%
            mutate(cPre = c(scale(Pre, scale=F))),
          by = join_by(School_ID, Student_ID)) %&gt;%
  filter(Year_level &gt; 3) %&gt;%
  lmer(Numeracy ~ Treatment*Year_level + cPre + (1|School_ID/Year_level),
       data = .) -&gt; fit.6

tidy(fit.6, effects = &quot;fixed&quot;) %&gt;%
  filter(term == &quot;Treatment1:Year_level&quot;) # 7.44 (1.72)</code></pre>
<pre><code>## # A tibble: 1 × 7
##   effect term                  estimate std.error statistic    df   p.value
##   &lt;chr&gt;  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
## 1 fixed  Treatment1:Year_level     7.44      1.72      4.33  102. 0.0000350</code></pre>
<pre class="r"><code># adding covariates does not change the estimate
left_join(df, 
          filter(df, Year_level==3) %&gt;% 
            select(School_ID, Student_ID, Pre = Numeracy) %&gt;%
            mutate(cPre = c(scale(Pre, scale=F))),
          by = join_by(School_ID, Student_ID)) %&gt;%
  filter(Year_level &gt; 3) %&gt;%
  lmer(Numeracy ~ Treatment*Year_level + cPre + SES + Gender + First_Nations + 
         Language_background + (1|School_ID/Year_level),
       data = .) -&gt; fit.7

tidy(fit.7, effects = &quot;fixed&quot;) %&gt;%
  filter(term == &quot;Treatment1:Year_level&quot;) # 7.44 (1.72)</code></pre>
<pre><code>## # A tibble: 1 × 7
##   effect term                  estimate std.error statistic    df   p.value
##   &lt;chr&gt;  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
## 1 fixed  Treatment1:Year_level     7.44      1.72      4.33  102. 0.0000350</code></pre>
<p>After realising the short-term gains, the average long-term gain of
program schools in Year 4 to Year 6 was 7.4 points year-to-year over
other schools.</p>
<p><br></p>
<p>The long-term gain could also be represented by the total relative
gain between Year 3 to Year 6 for the program schools over the other
schools. To estimate this, we include Year_level as a dummy variable
rather than as a linear term.</p>
<pre class="r"><code>mutate(df, Year_level = factor(Year_level)) %&gt;%
  lmer(Numeracy ~ Treatment*Year_level + (1|School_ID/Year_level),
       data = .) -&gt; fit.8</code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<pre class="r"><code>tidy(fit.8, effects = &quot;fixed&quot;) %&gt;%
  filter(str_detect(term, &quot;Treatment1:&quot;)) %&gt;%
  select(-effect) # 24.8 (3.77)</code></pre>
<pre><code>## # A tibble: 3 × 6
##   term                   estimate std.error statistic    df  p.value
##   &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1 Treatment1:Year_level4     9.50      3.77      2.52 7661. 1.18e- 2
## 2 Treatment1:Year_level5    19.8       3.77      5.25 7661. 1.60e- 7
## 3 Treatment1:Year_level6    24.8       3.77      6.58 7661. 4.98e-11</code></pre>
<p><br></p>
<p><br></p>
</div>
</div>
</div>
<div id="takeaways" class="section level1">
<h1>Takeaways</h1>
<p>In general gain score analyses answer a distinctly different question
from ANCOVA, namely “Do subjects in the treatment group gain more than
subjects in the control group?”, which is more appropriate when
randomization is not present.</p>
<p>Use ANCOVA (with or without change scores) when estimating <span
class="math inline">\(\tau\)</span> with random allocation to treatment
groups (RCT).</p>
<p>Without random allocation, use change scores to estimate gain (and
ANCOVA may be vulnerable to RTM).</p>
<p>Equating pre-existing differences is not always a good idea and can
produce bias in some causal effects (e.g., when the groups are the
reason for the pre-existing difference in Y)</p>
<p>When to use change-from-baseline (CFB or gain scores)<br />
<img src="PREvPOST.png" /></p>
<p><br></p>
<p><br></p>
</div>
<div id="not-run" class="section level1">
<h1>Not run</h1>
<pre class="r"><code># Adding covariates to equate for pre-existing differences does not change the
# location of the estimate, but does slightly improve the precision
mutate(df, Year_level = factor(Year_level)) %&gt;%
  lmer(Numeracy ~ Treatment*Year_level + SES + Gender + 
         First_Nations + Language_background + (1|School_ID/Year_level),
       data = .) -&gt; fit.9

tidy(fit.9, effects = &quot;fixed&quot;) %&gt;%
  filter(str_detect(term, &quot;Treatment1:&quot;)) %&gt;%
  select(-effect) # 24.8 (3.72)</code></pre>
<pre class="r"><code>p1 &lt;- ggpredict(fit.8, terms = c(&quot;Year_level&quot;, &quot;Treatment&quot;)) %&gt;% plot() +
  labs(subtitle = &quot;without adjustment&quot;)

p2 &lt;- ggpredict(fit.9, terms = c(&quot;Year_level&quot;, &quot;Treatment&quot;)) %&gt;% plot() +
  labs(subtitle = &quot;with adjustment&quot;)

p1 + p2 + plot_layout(guides = &#39;collect&#39;) &amp; 
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><br></p>
<pre class="r"><code># Assuming a smooth effect of time
mutate(df, Treatment = ordered(Treatment)) %&gt;%
  gamm(Numeracy ~ Treatment +s(Year_level, k = 4) + 
         s(Year_level, by = Treatment, k = 4),
       correlation = corAR1(form = ~1|School_ID),
       data = .) -&gt; fit

ggpredict(fit$gam, terms = c(&quot;Year_level&quot;, &quot;Treatment&quot;)) %&gt;% plot()



plot(fit$gam, scale=0, shade=T, select=2, seWithMean=T)
abline(h=0, lty = 2)</code></pre>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
